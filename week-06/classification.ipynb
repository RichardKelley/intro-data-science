{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Basic Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our goal in this notebook is to get a handle on the basic task of classification. To do this, we'll use scikit-learn to create some simple synthetic datasets, we'll use these datasets to train some models, and we'll see how the models transform inputs to outputs.\n",
      "\n",
      "The goal of machine learning is to automate prediction. We are surrounded by data, and as humans we make predictions based on this data all the time. Historically, people have been skittish about \"predicting the future,\" but in a lot of cases it's really not that hard. If we see a lot of data and notice that it conforms to a pattern, we can use our knowledge of that pattern to make predictions similar data we see in the future. In a sense, all of machine learning is just a matter of formalizing that intuition and expressing it in a way that's easy to write code for. The problem we're going to look at should walk you through the basics of how that's done:\n",
      "\n",
      "Let's start by importing what we need:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To make our predictions about data, we're going to build a classifier. A _classifier_ is a function $c : \\mathbb{R}^n \\rightarrow X$. If you've never seen this notation before, it's just an easy way to say the following: \"$c$ is a function. It takes inputs from the set $\\mathbb{R}^n$ and produces values from the set $X$.\" In other words, that notation is just the concise way that mathematicians write down the _types_ of their functions.\n",
      "\n",
      "The notation $\\mathbb{R}^n$ is short for \"vectors of length $n$, with components that are real numbers.\" So the input to a classifier is a vector of (for us) floating point numbers. Most of the basic machine learning algorithms assume that inputs appear in the form of floating-point vectors.\n",
      "\n",
      "In the case of classification, the set $X$ is a finite set of _labels_. In the simplest case, called _two-class classification_, the set has two labels as elements. Depending on the application, these labels could be \"spam\" and \"not spam\", in which case $X = \\{\"spam\", \"not spam\"\\}$. Or the labels could be \"happy\" and \"sad\", in which case $X = \\{\"happy\", \"sad\"\\}$. Of course, it's much simpler if instead of strings for labels we just use numbers, so usually we just say $X = \\{0, 1\\}$, and then somewhere else write down the relationship between the numbers and the strings we really care about.\n",
      "\n",
      "If we have a classifier $c$ available to us, we can use it to predict labels for inputs that we've never seen before (to a first approximation, this is what gmail is doing when it reads your incoming mail - using a spam classifier to decide whether or not to show you the incoming mail). In the real world, this is never what happens. Instead, if we're lucky, we get a bunch of data points with labels, and we need to _train_ a classifier using the available data and labels.\n",
      "\n",
      "(In the real real world, things are even worse. Usually you don't even get labels. And a lot of the time you're just handed a file of stuff, and you have to clean and transform the data to get something you can use to train a classifier. For now, we'll ignore those details.)\n",
      "\n",
      "The scikit-learn `datasets.make_classification` function gives this kind of data. Don't worry about what all the parameters below mean - just focus on the fact that `easy_x` is a $400 \\times 2$ matrix and `easy_y` is a $400 \\times 1$ column vector."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(easy_x, easy_y) = datasets.make_classification(n_samples=400, n_features = 2, n_informative = 2,\n",
      "                             n_redundant = 0, n_repeated = 0, n_clusters_per_class=1, class_sep=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print easy_x.shape\n",
      "print easy_y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To understand what `easy_x` and `easy_y` are doing, we can look at the first few rows of each:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print easy_x[0,:], easy_y[0]\n",
      "print easy_x[1,:], easy_y[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can read these lines as saying \"the point [..., ...] has the label ...\" and \"the point [..., ...] has the label ...\" So each row of the matrix `easy_x` is a data point, and the corresponding element of `easy_y` is that data point's label. \n",
      "\n",
      "Looking at the data in this case is really easy (each row is small, there aren't many labels, and there aren't that many rows). But you can really see what's going on if you visualize the data. Exploratory visualization is almost always a good idea. Let's plot the points with different colors for each of the two labels:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scatter(easy_x[:,0], easy_x[:,1], c = easy_y, cmap = 'cool')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Points that are close together get similar or identical labels. You can implement this directly using the idea of \"nearest neighbors,\" where you assign a label to a new point by looking at the $k$ nearest neighbors of the new point and let those neighbors \"vote\" on which label the point should get. Here's how that would work in scikit-learn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import neighbors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# these two lines create a classifier and train the classifier.\n",
      "n_neighbors = 10\n",
      "nn_classifier = neighbors.KNeighborsClassifier(n_neighbors, 'distance')\n",
      "nn_classifier.fit(easy_x, easy_y)\n",
      "\n",
      "# These seven lines show how points in a grid would be classified\n",
      "h = 0.05 # mesh size\n",
      "x_min, x_max = easy_x[:, 0].min() - 1, easy_x[:,0].max() + 1\n",
      "y_min, y_max = easy_x[:, 1].min() - 1, easy_x[:,1].max() + 1\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "Z = nn_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "Z = Z.reshape(xx.shape)\n",
      "figure()\n",
      "pcolormesh(xx,yy, Z, cmap='cool')\n",
      "\n",
      "# this line plots the training data\n",
      "scatter(easy_x[:,0], easy_x[:,1], c=easy_y, cmap='cool')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even though the classifier uses neighbor voting to decide how to classify points, in this case we can see that it has almost the same effect as drawing a line in the plane and using that line to perform classification. In a sense, this is how many machine learning systems operate. We can see this by looking at a slightly more sophisticated classifier, called _logistic regression_. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we fit the classifier, we can see what labels it will assign to new inputs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn_classifier.predict([2,-6])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "lr_classifier = LogisticRegression()\n",
      "lr_classifier.fit(easy_x, easy_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# just plotting...\n",
      "Z_lr = lr_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "Z_lr = Z_lr.reshape(xx.shape)\n",
      "figure()\n",
      "pcolormesh(xx,yy, Z_lr, cmap='cool')\n",
      "scatter(easy_x[:,0], easy_x[:,1], c=easy_y, cmap='cool')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Responsible Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have a classifier, we can run it as we did above to see how it performs on some data. But the approach that we used above has a serious problem - we have no way of knowing how well the classifier will perform if we use it in the field. How would you pick between the kNN classifier and logistic regression above? If the classes are easily separable it doesn't matter, but in the real world we can't just glance at the geometry to see how well the classes can be separated. We need something more quantitative, that will generalize to higher dimensions than two or three."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing you might try is only training on a fraction of the given data, using the rest for evaluation purposes. This approach, which is standard in the ML community, uses a _training set_ and a _test set_. Let's see how that works by generating a new data set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(new_x, new_y) = datasets.make_classification(n_samples=600, n_features = 2, n_informative = 2,\n",
      "                             n_redundant = 0, n_repeated = 0, n_clusters_per_class=1, class_sep=1.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To break this new data into training and test, let's just take the first 500 points for training and the last 100 for test:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_x, training_y = new_x[:500, :], new_y[:500]\n",
      "test_x, test_y = new_x[-100:, :], new_y[-100:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use the `score` function to see how well the classifier performs on the test data. Numbers close to 0 are bad and numbers close to 1 are good: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr2_clf = LogisticRegression()\n",
      "lr2_clf.fit(training_x, training_y).score(test_x, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lastly, we can generalize this training/test split to multiple folds using `cross_val_score`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "kfold = cross_validation.KFold(len(test_x), n_folds = 3)\n",
      "cross_validation.cross_val_score(lr2_clf, test_x, test_y, cv=kfold, n_jobs=-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}